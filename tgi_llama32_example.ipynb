{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ü¶ô **Using the Text Generation Inference (TGI) with Llama Models**\n",
        "\n",
        "In this notebook, we'll explore how simple it is to serve and consume the **[Llama models](https://huggingface.co/blog/llama32)** using the **Text Generation Inference (TGI)** project. üöÄ\n",
        "\n",
        "## üóÇ **Available Llama Models**\n",
        "\n",
        "You can browse the entire collection of Llama models over at [this link](https://huggingface.co/collections/meta-llama/llama-32-66f448ffc8c32f949b04c8cf). These models range in size and capability, giving you plenty of options for your text generation needs.\n",
        "\n",
        "## üìö **Learn More About TGI**\n",
        "\n",
        "To explore the technical details behind the **Text Generation Inference** project, visit the [official GitHub repository](https://github.com/huggingface/text-generation-inference). üí°\n",
        "\n"
      ],
      "metadata": {
        "id": "RXDFWABmtoLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üçΩÔ∏è **Serving the Llama Model**\n",
        "\n",
        "Now that we've seen the available models, it's time to **serve** one using the **Text Generation Inference (TGI)** framework! üõ†Ô∏è\n",
        "\n",
        "With TGI, you can deploy Llama models efficiently to handle text generation requests. Whether you're hosting it on your local machine, or deploying it on the cloud, the process is streamlined for performance and scalability.\n"
      ],
      "metadata": {
        "id": "X7YGZuhOi2A-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üê≥ **Using Docker**\n",
        "\n",
        "Check out the Docker setup guide in the official TGI repository [here](https://github.com/huggingface/text-generation-inference?tab=readme-ov-file#docker).\n",
        "\n"
      ],
      "metadata": {
        "id": "aImxZUz5j7aA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "model=meta-llama/Llama-3.2-1B\n",
        "# share a volume with the Docker container to avoid downloading weights every run\n",
        "volume=$PWD/data\n",
        "token=<HF_TOKEN>\n",
        "\n",
        "docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data -e HF_TOKEN=$token \\\n",
        "  ghcr.io/huggingface/text-generation-inference:2.3.0 --model-id $model --quantize bitsandbytes\n",
        "'''"
      ],
      "metadata": {
        "id": "5kJVzIJ-jotP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can call the model using CURL:"
      ],
      "metadata": {
        "id": "KpVvszxyjQJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "curl 127.0.0.1:8080/generate_stream \\\n",
        "  -X POST  \\\n",
        "  -d '{\"inputs\":\"What is Deep Learning?\",\"parameters\":{\"max_new_tokens\":20}}'  \\\n",
        "  -H 'Content-Type: application/json'\n",
        "'''"
      ],
      "metadata": {
        "id": "3Rw93B4BynMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üåê **Using Inference Endpoints**\n",
        "\n",
        "Another efficient way to serve Llama models is by using **Inference Endpoints** on Hugging Face. üöÄ This allows you to deploy models in a fully managed environment with just a few clicks.\n",
        "\n",
        "For detailed instructions on how to set up and use Inference Endpoints, refer to the [official documentation](https://huggingface.co/docs/inference-endpoints/index). You'll find everything you need to start serving your models in a reliable and scalable way! üí°\n"
      ],
      "metadata": {
        "id": "bkYYG-Hgj883"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üçΩÔ∏è **Consuming the Model**\n",
        "\n",
        "Once the Llama model is up and running, you can start interacting with it using a simple API.\n",
        "\n",
        "To consume the model, you'll send a request to the API endpoint and receive a response with the generated text. Hugging Face provides an easy-to-use interface for this, making it accessible from any application.\n",
        "\n",
        "You can refer to the [API Inference Notebook](https://github.com/huggingface/huggingface-llama-recipes/blob/ae10e290a3bf1cbdc8523b3eb5ac2437f09e0877/api_inference/inference-api.ipynb) for a step-by-step guide on how to send requests to the model and retrieve responses. This notebook provides sample code and instructions to get you up and running quickly ü¶ô‚ú®.\n"
      ],
      "metadata": {
        "id": "huozmkWtuGIs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üêç **Using the Python API**\n",
        "\n",
        "To interact with the Llama models programmatically, you can utilize the **[huggingface_hub's Inference Client](https://huggingface.co/docs/huggingface_hub/guides/inference)**.\n",
        "\n"
      ],
      "metadata": {
        "id": "WuQZErhAfOpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "KarzRgNVj2d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This models are gated so we need to authenticate first"
      ],
      "metadata": {
        "id": "bjaE-uKnnZsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "z_QYU3GXjz59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úçÔ∏è **Text Generation**\n",
        "\n",
        "Using the Hugging Face Hub, you can easily perform text generation with the Llama models. Below is an example of how to utilize the `InferenceClient` to generate text based on a given prompt. üöÄ\n"
      ],
      "metadata": {
        "id": "6g6kaQqugjQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "client = InferenceClient(model=\"meta-llama/Llama-3.2-1B\")\n",
        "\n",
        "response = client.text_generation(\n",
        "    prompt=\"A HTTP POST request is used to \",\n",
        "    temperature=0.8,\n",
        "    max_new_tokens=50,\n",
        "    seed=42,\n",
        "    return_full_text=True,\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XojTZswO2K41",
        "outputId": "7cffd781-1b9e-49aa-d0c8-16daeab6d21f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A HTTP POST request is used to ¬†send a new entity to a web server. There are many reasons why you may want to send out a new entity, such as: creating a new user, making changes to an existing user, or sending in new orders.¬†This tutorial will\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üí¨ **Chat Example**\n",
        "\n",
        "The Llama models can also be utilized for chat-like interactions. With the `InferenceClient`, you can easily create conversational AI experiences. Below is an example of how to generate a chat response based on user input. üó£Ô∏è\n"
      ],
      "metadata": {
        "id": "NiQzlbJ9gnNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = InferenceClient(model=\"meta-llama/Llama-3.2-11B-Vision-Instruct\")\n",
        "\n",
        "output = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Count to 10\"},\n",
        "    ],\n",
        "    stream=True,\n",
        "    max_tokens=1024,\n",
        ")\n",
        "\n",
        "for chunk in output:\n",
        "    print(chunk.choices[0].delta.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKZedubstOO3",
        "outputId": "5422d3f9-587e-4763-a76f-b3d845de2513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            ",\n",
            " \n",
            "2\n",
            ",\n",
            " \n",
            "3\n",
            ",\n",
            " \n",
            "4\n",
            ",\n",
            " \n",
            "5\n",
            ",\n",
            " \n",
            "6\n",
            ",\n",
            " \n",
            "7\n",
            ",\n",
            " \n",
            "8\n",
            ",\n",
            " \n",
            "9\n",
            ",\n",
            " \n",
            "10\n",
            ".\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üñºÔ∏è **Chat with Image**\n",
        "\n",
        "The Llama models also support multimodal interactions, allowing you to send images along with text prompts. This feature enables the model to analyze images and respond accordingly. Below is an example of how to engage in a chat with an image input. üì∏\n"
      ],
      "metadata": {
        "id": "F9cMGKvNgpEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = InferenceClient(model=\"meta-llama/Llama-3.2-11B-Vision-Instruct\")\n",
        "\n",
        "output = client.chat.completions.create(\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "          {\"type\": \"text\", \"text\": \"What‚Äôs in this image?\"},\n",
        "          {\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": {\n",
        "              \"url\": \"https://raw.githubusercontent.com/haotian-liu/LLaVA/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg\",\n",
        "            },\n",
        "          },\n",
        "        ],\n",
        "      }\n",
        "    ],\n",
        "    stream=True,\n",
        "    max_tokens=200,\n",
        ")\n",
        "\n",
        "full_response = []\n",
        "for chunk in output:\n",
        "    full_response.append(chunk.choices[0].delta.content)\n",
        "\n",
        "final_text = ''.join(full_response)\n",
        "print(final_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVnIAJq5Rlan",
        "outputId": "0943e392-0f8c-45b2-e511-e4257b2d00ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The image is a graph showing the performance of several systems, each identified by a name consisting of four letters. The x-axis represents the name of each system, while the y-axis denotes the performance metric being measured. The graph features a range of colors, with the lines varying in thickness.\n",
            "\n",
            "The graph is set against a white background, which provides a clean and neutral backdrop for the information being presented. Overall, the graph effectively communicates the performance of each system, allowing for easy comparison and analysis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì∏ **Chat with an Image in Base64 Format**\n",
        "\n",
        "You can also send images encoded in Base64 format to the Llama models, enabling multimodal interactions without relying on image URLs. This approach can be useful when you want to embed images directly in your requests. Below is an example of how to use Base64 encoding for image inputs in a chat. üåü\n"
      ],
      "metadata": {
        "id": "kg1A1sIMgr1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "url = 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg'\n",
        "\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "def encode_image(image):\n",
        "    buffered = BytesIO()\n",
        "    image.save(buffered, format=\"JPEG\")\n",
        "    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
        "\n",
        "base64_image = encode_image(image)\n",
        "\n",
        "output = client.chat.completions.create(\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "          {\"type\": \"text\", \"text\": \"What‚Äôs in this image?\"},\n",
        "          {\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": {\n",
        "              \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "            },\n",
        "          },\n",
        "        ],\n",
        "      }\n",
        "    ],\n",
        "    stream=True,\n",
        "    max_tokens=200,\n",
        ")\n",
        "\n",
        "full_response = []\n",
        "for chunk in output:\n",
        "    full_response.append(chunk.choices[0].delta.content)\n",
        "\n",
        "final_text = ''.join(full_response)\n",
        "print(final_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbH682zbauc_",
        "outputId": "a659704c-9ba3-41e4-f5bd-5e913695ef5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The image depict a rabbit in clothing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üåê **Using CURL**\n",
        "\n",
        "You can also interact with the ü¶ô Llama models using CURL. Below are examples of how to use CURL for both standard text generation and chat completions.\n",
        "\n",
        "_Authorization Token: Replace `<Token>` with your actual Hugging Face API token._\n",
        "\n",
        "### 1. **Text Generation**\n",
        "\n",
        "To generate text using the Llama model, you can send a POST request like this:\n"
      ],
      "metadata": {
        "id": "-weBarUKfRbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-11B-Vision-Instruct -X POST \\\n",
        "    -d '{\"inputs\":\"What is Deep Learning?\",\"parameters\":{\"max_new_tokens\":20}}' \\\n",
        "    -H 'Content-Type: application/json'\\\n",
        "    -H \"Authorization: Bearer <Token>\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtyLuXDUf8WZ",
        "outputId": "6a75f741-be65-4092-8108-4bf112cd53d5"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{\"generated_text\":\" A Beginner‚Äôs Guide\\nDeep learning is a subset of machine learning that involves the use of artificial neural\"}]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. **Chat Completions**\n",
        "\n",
        "For chat interactions, you can use a similar approach to send messages:"
      ],
      "metadata": {
        "id": "8aLIJW04kSla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-11B-Vision-Instruct/v1/chat/completions -X POST \\\n",
        "    -d '{\"messages\": [{\"role\": \"system\",\"content\": \"You are a helpful assistant.\"},{\"role\": \"user\",\"content\": \"What is deep learning?\"}],\"stream\": true,\"max_tokens\": 20}' \\\n",
        "    -H 'Content-Type: application/json' \\\n",
        "    -H \"Authorization: Bearer <Token>\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUP93Ekohc5V",
        "outputId": "04cc9150-8c09-4388-a7a7-881f3eecc156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data: {\"object\":\"chat.completion.chunk\",\"id\":\"\",\"created\":1727810075,\"model\":\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\"system_fingerprint\":\"2.3.1-dev0-sha-de90261\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"Deep\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n",
            "\n",
            "data: {\"object\":\"chat.completion.chunk\",\"id\":\"\",\"created\":1727810075,\"model\":\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\"system_fingerprint\":\"2.3.1-dev0-sha-de90261\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\" learning\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n",
            "\n",
            "data: {\"object\":\"chat.completion.chunk\",\"id\":\"\",\"created\":1727810075,\"model\":\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\"system_fingerprint\":\"2.3.1-dev0-sha-de90261\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\" is\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n",
            "\n",
            "data: {\"object\":\"chat.completion.chunk\",\"id\":\"\",\"created\":1727810075,\"model\":\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\"system_fingerprint\":\"2.3.1-dev0-sha-de90261\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\" a\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n",
            "\n",
            "data: {\"object\":\"chat.completion.chunk\",\"id\":\"\",\"created\":1727810075,\"model\":\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\"system_fingerprint\":\"2.3.1-dev0-sha-de90261\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\" subset\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n",
            "\n",
            "data: {\"object\":\"chat.completion.chunk\",\"id\":\"\",\"created\":1727810075,\"model\":\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\"system_fingerprint\":\"2.3.1-dev0-sha-de90261\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\" of\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n",
            "\n",
            "data: {\"object\":\"chat.completion.chunk\",\"id\":\"\",\"created\":1727810075,\"model\":\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\"system_fingerprint\":\"2.3.1-dev0-sha-de90261\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\" machine\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n",
            "\n",
            "data: {\"object\":\"chat.completion.chunk\",\"id\":\"\",\"created\":1727810075,\"model\":\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\"system_fingerprint\":\"2.3.1-dev0-sha-de90261\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\" learning\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n",
            "\n",
            "data: {\"object\":\"chat.completion.chunk\",\"id\":\"\",\"created\":1727810075,\"model\":\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\"system_fingerprint\":\"2.3.1-dev0-sha-de90261\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\" (\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n",
            "\n",
            "data: {\"object\":\"chat.completion.chunk\",\"id\":\"\",\"created\":1727810075,\"model\":\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\"system_fingerprint\":\"2.3.1-dev0-sha-de90261\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"ML\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n",
            "\n",
            "data: {\"object\":\"chat.completion.chunk\",\"id\":\"\",\"created\":1727810075,\"model\":\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\"system_fingerprint\":\"2.3.1-dev0-sha-de90261\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\")\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n",
            "\n",
            "data: {\"object\":\"chat.completion.chunk\",\"id\":\"\",\"created\":1727810075,\"model\":\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\"system_fingerprint\":\"2.3.1-dev0-sha-de90261\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\" that\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n",
            "\n",
            "data: {\"object\":\"chat.completion.chunk\",\"id\":\"\",\"created\":1727810075,\"model\":\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\"system_fingerprint\":\"2.3.1-dev0-sha-de90261\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\" uses\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n",
            "\n",
            "data: {\"object\":\"chat.completion.chunk\",\"id\":\"\",\"created\":1727810075,\"model\":\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\"system_fingerprint\":\"2.3.1-dev0-sha-de90261\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\" artificial\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n",
            "\n",
            "data: {\"object\":\"chat.completion.chunk\",\"id\":\"\",\"created\":1727810075,\"model\":\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\"system_fingerprint\":\"2.3.1-dev0-sha-de90261\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\" neural\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n",
            "\n",
            "data: {\"object\":\"chat.completion.chunk\",\"id\":\"\",\"created\":1727810075,\"model\":\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\"system_fingerprint\":\"2.3.1-dev0-sha-de90261\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\" networks\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n",
            "\n",
            "data: {\"object\":\"chat.completion.chunk\",\"id\":\"\",\"created\":1727810075,\"model\":\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\"system_fingerprint\":\"2.3.1-dev0-sha-de90261\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\" (\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n",
            "\n",
            "data: {\"object\":\"chat.completion.chunk\",\"id\":\"\",\"created\":1727810075,\"model\":\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\"system_fingerprint\":\"2.3.1-dev0-sha-de90261\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"ANN\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n",
            "\n",
            "data: {\"object\":\"chat.completion.chunk\",\"id\":\"\",\"created\":1727810075,\"model\":\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\"system_fingerprint\":\"2.3.1-dev0-sha-de90261\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"s\"},\"logprobs\":null,\"finish_reason\":null}],\"usage\":null}\n",
            "\n",
            "data: {\"object\":\"chat.completion.chunk\",\"id\":\"\",\"created\":1727810075,\"model\":\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\"system_fingerprint\":\"2.3.1-dev0-sha-de90261\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\")\"},\"logprobs\":null,\"finish_reason\":\"length\"}],\"usage\":null}\n",
            "\n",
            "data: [DONE]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/api-inference/parameters"
      ],
      "metadata": {
        "id": "_vF9XLItpoYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
        "headers = {\n",
        "    \"Authorization\": \"Bearer <token>\",\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"x-use-cache\": \"false\"\n",
        "}\n",
        "data = {\n",
        "    \"inputs\": \"What is Deep Learning?\"\n",
        "}\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "print(response.json())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtTPYW-tpHQ0",
        "outputId": "99788c22-8006-43b6-9df8-44083f72bf16"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': \" [7 models explained]\\nPosted by: techsaiyan in Machine Learning June 2, 2019\\nDeep Learning is a branch of artificial intelligence that ensues the development of a computer's potential to interpret, research, and understand complex data like photos, audio, and texts. The ultimate objective of deep learning frameworks is to influence machines to examine, process raw data and extract valuable information from the data. Deep learning exists within machine learning and all ensues the development of a computer's potential\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-11B-Vision-Instruct/v1/chat/completions\"\n",
        "headers = {\n",
        "    \"Authorization\": \"Bearer <token>\",\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"x-use-cache\": \"false\"\n",
        "}\n",
        "\n",
        "data = {\n",
        "    \"messages\":  [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is deep learning?\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, json=data)\n",
        "print(response.json())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2aH1v5ZpOVi",
        "outputId": "10df8027-f0ea-4c68-b189-5943dbae717b"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'object': 'chat.completion', 'id': '', 'created': 1727812546, 'model': 'meta-llama/Llama-3.2-11B-Vision-Instruct', 'system_fingerprint': '2.3.1-dev0-sha-de90261', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Deep learning is a subfield of machine learning that involves the use of artificial neural networks (ANNs) with multiple layers to learn and represent data. The main idea behind deep learning is to create models that can learn complex patterns and features from large amounts of data by mimicking the structure and function of the human brain.\\n\\nIn traditional machine learning, models are designed to recognize and classify patterns using hand-engineered features. However, deep learning models learn these features automatically from the data, which allows them to'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 47, 'completion_tokens': 100, 'total_tokens': 147}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cDfECfRNpxCW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}